{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c990a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af907b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Colors from Colorbrewer Paired_12\n",
    "colors = [[31, 120, 180], [51, 160, 44], [250,159,181]]\n",
    "colors = [(r / 255, g / 255, b / 255) for (r, g, b) in colors]\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img, norm):\n",
    "    \"\"\"\n",
    "    :param img: (PyTorch Tensor)\n",
    "    \"\"\"\n",
    "    if norm:\n",
    "        # unnormalize\n",
    "        img = img / 2 + 0.5    \n",
    "        \n",
    "    # Convert tensor to numpy array\n",
    "    npimg = img.numpy()\n",
    "    # Color channel first -> color channel last\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def get_image(img):\n",
    "    \"\"\"\n",
    "    convert a tensor to images suitable for plotting\n",
    "    :param img: (PyTorch Tensor)\n",
    "    \"\"\"\n",
    "    # unnormalize\n",
    "    img = img / 2 + 0.5     \n",
    "    # Convert tensor to numpy array\n",
    "    npimg = img.numpy()\n",
    "    # Color channel first -> color channel last\n",
    "    return np.transpose(npimg, (1, 2, 0))\n",
    "\n",
    "def plot_loss_acc(train_loss, val_loss, train_acc, val_acc):\n",
    "    x = np.arange(1, len(train_loss) + 1)\n",
    "\n",
    "    fig,axs = plt.subplots(1, 2, figsize=(16,6))\n",
    "    axs[0].plot(x, train_loss, color=colors[0], label=\"Training loss\", linewidth=2)\n",
    "    axs[0].plot(x, val_loss, color=colors[1], label=\"Validation loss\", linewidth=2)\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend(loc='upper right')\n",
    "    axs[0].set_title(\"Evolution of the training, validation and test loss\")\n",
    "\n",
    "    axs[1].plot(x, train_acc, color=colors[0], label=\"Training accuracy\", linewidth=2)\n",
    "    axs[1].plot(x, val_acc, color=colors[1], label=\"Validation accuracy\", linewidth=2)\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend(loc='lower right')\n",
    "    axs[1].set_title(\"Evolution of the training, validation and test accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_loss(train_loss, val_loss):\n",
    "    x = np.arange(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, train_loss, color=colors[0], label=\"Training loss\", linewidth=2)\n",
    "    plt.plot(x, val_loss, color=colors[1], label=\"Validation loss\", linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"Evolution of the training, validation and test loss\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_pvals(pvals, labels):\n",
    "    idx_in = np.where(labels==0)[0]\n",
    "    idx_out = np.where(labels==1)[0]\n",
    "\n",
    "    pvals_in = np.array(pvals)[idx_in]\n",
    "    pvals_out = np.array(pvals)[idx_out]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.hist(pvals_in, bins=100, alpha=0.5, label=\"inliers\")\n",
    "    plt.hist(pvals_out, bins=100, alpha=0.5, label=\"outliers\")\n",
    "    plt.legend()\n",
    "    plt.show\n",
    "\n",
    "    print('Average p-value for inliers is {:3f}, average p-value for outliers is {:3f}.'\\\n",
    "          .format(np.mean(pvals_in), np.mean(pvals_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8f3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "sys.path.append('../ConformalizedES')\n",
    "sys.path.append('../third_party')\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b1132",
   "metadata": {},
   "source": [
    "# Experiment: Compare with the naive benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef027e",
   "metadata": {},
   "source": [
    "### Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0c50ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MNIST Dataset\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#      transforms.Normalize(mean=0.5, std=0.5)])\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "train_set_full = datasets.MNIST(root = \"./data\", train = True, download = True, transform=transform)\n",
    "test_set_full = datasets.MNIST(root = \"./data\", train = False, download = True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd1674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of available training data is: 5923.\n",
      "total number of test data is 1954 in which 980 are label 0 test data, 974 are label 8 test data.\n"
     ]
    }
   ],
   "source": [
    "from datasetMaker import get_class_i, DatasetMaker\n",
    "\n",
    "x_train_full = train_set_full.data\n",
    "y_train_full = train_set_full.targets\n",
    "x_test_full = test_set_full.data\n",
    "y_test_full = test_set_full.targets\n",
    "\n",
    "# Train set composed only of number 0\n",
    "train_set = \\\n",
    "    DatasetMaker(\n",
    "        [get_class_i(x_train_full, y_train_full, 0)]\n",
    "    )\n",
    "\n",
    "# Test set is a mixture of number 0 and 8\n",
    "test_set = \\\n",
    "    DatasetMaker(\n",
    "        [get_class_i(x_test_full, y_test_full, 0),\n",
    "        get_class_i(x_test_full, y_test_full, 8),\n",
    "]\n",
    "    )\n",
    "\n",
    "print('total number of available training data is: {:d}.'.format(len(train_set)))\n",
    "print('total number of test data is {:d} in which {:d} are label 0 test data, {:d} are label 8 test data.'\\\n",
    "      .format(len(test_set), test_set.lengths[0],test_set.lengths[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86fabe3",
   "metadata": {},
   "source": [
    "### Benchmark data-splitting vs CES data-splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11925c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "def split_data(seed, n_train_bm, n_val_bm, n_cal_bm, batch_size=10, num_workers=0):\n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "\n",
    "    # MNIST is a simple dataset, hence we only take 300 inlier data to demonstrate\n",
    "    # Benchmark data splitting: equally split the data into 3 sets\n",
    "    n_full = len(train_set)\n",
    "    n_data = n_train_bm + n_val_bm + n_cal_bm\n",
    "\n",
    "    train_set_bm, val_set_bm, cal_set_bm, _ = th.utils.data.random_split(train_set,\\\n",
    "                                     [n_train_bm, n_val_bm, n_cal_bm, n_full-n_data])\n",
    "\n",
    "    # CES data splitting: calibration set is not needed, merge back to the training set\n",
    "    n_train_ces = n_train_bm + n_val_bm\n",
    "    n_val_ces = n_cal_bm\n",
    "\n",
    "    train_set_ces, val_set_ces, _ = th.utils.data.random_split(train_set,\\\n",
    "                                     [n_train_ces, n_val_ces, n_full-n_data])\n",
    "    \n",
    "    train_loader_bm = th.utils.data.DataLoader(train_set_bm, batch_size=batch_size,\n",
    "                                          num_workers=num_workers)\n",
    "\n",
    "    val_loader_bm = th.utils.data.DataLoader(val_set_bm, batch_size=n_val_bm,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "    cal_loader_bm = th.utils.data.DataLoader(cal_set_bm, batch_size=n_cal_bm,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "    # For CES\n",
    "    train_loader_ces = th.utils.data.DataLoader(train_set_ces, batch_size=batch_size,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "    val_loader_ces = th.utils.data.DataLoader(val_set_ces, batch_size=n_val_ces,\n",
    "                                              num_workers=num_workers)\n",
    "    return train_loader_bm, val_loader_bm, cal_loader_bm, train_loader_ces, val_loader_ces\n",
    "\n",
    "def load_test(seed, n_test, num_workers=0):\n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "\n",
    "    test_sampler = SubsetRandomSampler(np.arange(n_test, dtype=np.int64))\n",
    "    test_loader = th.utils.data.DataLoader(test_set, batch_size=n_test, sampler=test_sampler,\n",
    "                                             num_workers=num_workers)\n",
    "\n",
    "    # get all test images\n",
    "    dataiter = iter(test_loader)\n",
    "    inputs, labels = dataiter.next()\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b3078",
   "metadata": {},
   "source": [
    "## Define experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b20f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from method import CES_oneClass\n",
    "from networks import ConvAutoencoder\n",
    "from utils import eval_pvalues\n",
    "import torch.optim as optim\n",
    "from inference import Conformal_PVals\n",
    "\n",
    "\n",
    "def run_experiment(seed, n_train_bm, n_val_bm, n_cal_bm, n_test, lr, n_epoch, alpha_list, batch_size=10, num_worker=0, visualize=True):\n",
    "    # Initialize result data frame\n",
    "    results = pd.DataFrame({})\n",
    "    \n",
    "    # Get the dataloaders and test points\n",
    "    train_loader_bm, val_loader_bm, cal_loader_bm, train_loader_ces, val_loader_ces =\\\n",
    "    split_data(seed, n_train_bm, n_val_bm, n_cal_bm, batch_size, num_worker)\n",
    "    inputs, labels = load_test(seed, n_test)\n",
    "    \n",
    "    \n",
    "    # Define default device, we should use the GPU (cuda) if available\n",
    "    device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")### Define subset of the dataset (so it is faster to train)\n",
    "    if th.cuda.is_available():\n",
    "        # Make CuDNN Determinist\n",
    "        th.backends.cudnn.deterministic = True\n",
    "        th.cuda.manual_seed(seed)\n",
    "    # Set the NN parameters \n",
    "    net_bm = ConvAutoencoder()\n",
    "    Loss = th.nn.MSELoss()\n",
    "    def criterion(outputs, inputs, targets):\n",
    "        return Loss(outputs, inputs)\n",
    "    optimizer_bm = optim.Adam(net_bm.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    \n",
    "    # Train with benchmark data splitting\n",
    "    print(\"Training with standard data splitting...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    CES_oc_bm = CES_oneClass(net_bm, device, train_loader_bm, batch_size=batch_size, max_epoch=n_epoch, \n",
    "                        learning_rate=lr, val_loader=val_loader_bm, criterion=criterion,optimizer=optimizer_bm)\n",
    "    CES_oc_bm.full_train(save_dir = './models/oneClass/exp'+str(seed)+'/benchmarks/', save_every = 1)\n",
    "    \n",
    "    if visualize:\n",
    "        plot_loss(CES_oc_bm.train_loss_history, CES_oc_bm.val_loss_history)\n",
    "    \n",
    "    # Compute the benchmark p-values\n",
    "    print('Computing standard benchmark p-values for {:d} test points...'.format(n_test))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    best_loss_bm, best_model_bm, val_loss_history_bm = CES_oc_bm.select_model()\n",
    "    model_list_bm = CES_oc_bm.model_list\n",
    "    C_PVals_bm = Conformal_PVals(net_bm, device, cal_loader_bm, model_list_bm, random_state = seed)\n",
    "    pvals_bm = C_PVals_bm.compute_pvals(inputs, [best_model_bm]*len(inputs))\n",
    "    results_bm = eval_pvalues(pvals_bm, labels, alpha_list)\n",
    "    results_bm[\"Method\"] = \"Standard benchmark\"\n",
    "    results = pd.concat([results, results_bm])\n",
    "    \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "\n",
    "    net_ces = ConvAutoencoder()\n",
    "    optimizer_ces = optim.Adam(net_ces.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize the CES class with model parameters\n",
    "    print(\"Training with CES data splitting...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    CES_oc_ces = CES_oneClass(net_ces, device, train_loader_ces, batch_size=batch_size, max_epoch=n_epoch, \n",
    "                            learning_rate=lr, val_loader=val_loader_ces, criterion=criterion,optimizer=optimizer_ces)\n",
    "    CES_oc_ces.full_train(save_dir = './models/oneClass/exp'+str(seed)+'/ces/', save_every = 1)\n",
    "    \n",
    "    if visualize:\n",
    "        plot_loss(CES_oc_ces.train_loss_history, CES_oc_ces.val_loss_history)\n",
    "    \n",
    "    model_list_ces = CES_oc_ces.model_list\n",
    "    C_PVals_ces = Conformal_PVals(net_ces, device, val_loader_ces, model_list_ces, random_state = seed)\n",
    "    \n",
    "    # Compute the benchmark p-values\n",
    "    print('Computing naive benchmark p-values for {:d} test points...'.format(n_test))\n",
    "    best_loss_naive, best_model_naive, val_loss_history_naive = CES_oc_ces.select_model()\n",
    "    pvals_naive = C_PVals_ces.compute_pvals(inputs, [best_model_naive]*len(inputs))\n",
    "    results_naive = eval_pvalues(pvals_naive, labels, alpha_list)\n",
    "    results_naive[\"Method\"] = \"Naive benchmark\"\n",
    "    results = pd.concat([results, results_naive])\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3265b6fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting repetition 1 of 10:\n",
      "\n",
      "Training with standard data splitting...\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 10\n",
      "n_epochs= 50\n",
      "learning_rate= 1\n",
      "==============================\n",
      "Epoch 1 of 50, 10% \t train_loss: 0.18  took: 0.03s\n",
      "Epoch 1 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 1 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 1 of 50, 40% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 1 of 50, 50% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 1 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 1 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 1 of 50, 80% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 1 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 1 of 50, 100% \t train_loss: 0.14  took: 0.03s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 1.\n",
      "Epoch 2 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 2 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 2 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 2 of 50, 40% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 2 of 50, 50% \t train_loss: 0.14  took: 0.02s\n",
      "Epoch 2 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 2 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 2 of 50, 80% \t train_loss: 0.14  took: 0.02s\n",
      "Epoch 2 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 2 of 50, 100% \t train_loss: 0.14  took: 0.03s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 2.\n",
      "Epoch 3 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 3 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 3 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 3 of 50, 40% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 3 of 50, 50% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 3 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 3 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 3 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 3 of 50, 90% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 3 of 50, 100% \t train_loss: 0.14  took: 0.03s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 3.\n",
      "Epoch 4 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 4 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 4 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 4 of 50, 40% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 4 of 50, 50% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 4 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 4 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 4 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 4 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 4 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 4.\n",
      "Epoch 5 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 5 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 5 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 5 of 50, 40% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 5 of 50, 50% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 5 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 5 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 5 of 50, 80% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 5 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 5 of 50, 100% \t train_loss: 0.14  took: 0.03s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 5.\n",
      "Epoch 6 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 6 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 6 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 6 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 6 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 6 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 6 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 6 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 6 of 50, 90% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 6 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 6.\n",
      "Epoch 7 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 7 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 7 of 50, 30% \t train_loss: 0.13  took: 0.05s\n",
      "Epoch 7 of 50, 40% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 7 of 50, 50% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 7 of 50, 60% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 7 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 7 of 50, 80% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 7 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 7 of 50, 100% \t train_loss: 0.14  took: 0.05s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 7.\n",
      "Epoch 8 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 8 of 50, 20% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 8 of 50, 30% \t train_loss: 0.13  took: 0.05s\n",
      "Epoch 8 of 50, 40% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 8 of 50, 50% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 8 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 8 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 8 of 50, 80% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 8 of 50, 90% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 8 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 8.\n",
      "Epoch 9 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 9 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 9 of 50, 30% \t train_loss: 0.13  took: 0.05s\n",
      "Epoch 9 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 9 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 9 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 9 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 9 of 50, 80% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 9 of 50, 90% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 9 of 50, 100% \t train_loss: 0.14  took: 0.07s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 9.\n",
      "Epoch 10 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 10 of 50, 20% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 10 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 10 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 10 of 50, 50% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 10 of 50, 60% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 10 of 50, 70% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 10 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 10 of 50, 90% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 10 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 10.\n",
      "Epoch 11 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 11 of 50, 20% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 11 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 11 of 50, 40% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 11 of 50, 50% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 11 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 11 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 11 of 50, 80% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 11 of 50, 90% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 11 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 11.\n",
      "Epoch 12 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 12 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 12 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 12 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 12 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 12 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 12 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 12 of 50, 80% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 12 of 50, 90% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 12 of 50, 100% \t train_loss: 0.14  took: 0.05s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 12.\n",
      "Epoch 13 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 13 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 13 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 13 of 50, 40% \t train_loss: 0.14  took: 0.06s\n",
      "Epoch 13 of 50, 50% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 13 of 50, 60% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 13 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 13 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 13 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 13 of 50, 100% \t train_loss: 0.14  took: 0.03s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 13.\n",
      "Epoch 14 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 14 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 14 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 14 of 50, 40% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 14 of 50, 50% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 14 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 14 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 14 of 50, 80% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 14 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 14 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 14.\n",
      "Epoch 15 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 15 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 15 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 15 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 15 of 50, 50% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 15 of 50, 60% \t train_loss: 0.16  took: 0.02s\n",
      "Epoch 15 of 50, 70% \t train_loss: 0.15  took: 0.05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 15 of 50, 90% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 15 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 15.\n",
      "Epoch 16 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 16 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 16 of 50, 30% \t train_loss: 0.13  took: 0.05s\n",
      "Epoch 16 of 50, 40% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 16 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 16 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 16 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 16 of 50, 80% \t train_loss: 0.14  took: 0.02s\n",
      "Epoch 16 of 50, 90% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 16 of 50, 100% \t train_loss: 0.14  took: 0.03s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 16.\n",
      "Epoch 17 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 17 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 17 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 17 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 17 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 17 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 17 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 17 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 17 of 50, 90% \t train_loss: 0.15  took: 0.02s\n",
      "Epoch 17 of 50, 100% \t train_loss: 0.14  took: 0.05s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 17.\n",
      "Epoch 18 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 18 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 18 of 50, 30% \t train_loss: 0.13  took: 0.05s\n",
      "Epoch 18 of 50, 40% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 18 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 18 of 50, 60% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 18 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 18 of 50, 80% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 18 of 50, 90% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 18 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 18.\n",
      "Epoch 19 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 19 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 19 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 19 of 50, 40% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 19 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 19 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 19 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 19 of 50, 80% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 19 of 50, 90% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 19 of 50, 100% \t train_loss: 0.14  took: 0.03s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 19.\n",
      "Epoch 20 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 20 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 20 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 20 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 20 of 50, 50% \t train_loss: 0.14  took: 0.02s\n",
      "Epoch 20 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 20 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 20 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 20 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 20 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 20.\n",
      "Epoch 21 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 21 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 21 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 21 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 21 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 21 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 21 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 21 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 21 of 50, 90% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 21 of 50, 100% \t train_loss: 0.14  took: 0.03s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 21.\n",
      "Epoch 22 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 22 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 22 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 22 of 50, 40% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 22 of 50, 50% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 22 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 22 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 22 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 22 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 22 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 22.\n",
      "Epoch 23 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 23 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 23 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 23 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 23 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 23 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 23 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 23 of 50, 80% \t train_loss: 0.14  took: 0.02s\n",
      "Epoch 23 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 23 of 50, 100% \t train_loss: 0.14  took: 0.03s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 23.\n",
      "Epoch 24 of 50, 10% \t train_loss: 0.15  took: 0.06s\n",
      "Epoch 24 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 24 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 24 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 24 of 50, 50% \t train_loss: 0.14  took: 0.02s\n",
      "Epoch 24 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 24 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 24 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 24 of 50, 90% \t train_loss: 0.15  took: 0.02s\n",
      "Epoch 24 of 50, 100% \t train_loss: 0.14  took: 0.05s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 24.\n",
      "Epoch 25 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 25 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 25 of 50, 30% \t train_loss: 0.13  took: 0.05s\n",
      "Epoch 25 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 25 of 50, 50% \t train_loss: 0.14  took: 0.02s\n",
      "Epoch 25 of 50, 60% \t train_loss: 0.16  took: 0.07s\n",
      "Epoch 25 of 50, 70% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 25 of 50, 80% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 25 of 50, 90% \t train_loss: 0.15  took: 0.06s\n",
      "Epoch 25 of 50, 100% \t train_loss: 0.14  took: 0.05s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 25.\n",
      "Epoch 26 of 50, 10% \t train_loss: 0.15  took: 0.02s\n",
      "Epoch 26 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 26 of 50, 30% \t train_loss: 0.13  took: 0.06s\n",
      "Epoch 26 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 26 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 26 of 50, 60% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 26 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 26 of 50, 80% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 26 of 50, 90% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 26 of 50, 100% \t train_loss: 0.14  took: 0.03s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 26.\n",
      "Epoch 27 of 50, 10% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 27 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 27 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 27 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 27 of 50, 50% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 27 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 27 of 50, 70% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 27 of 50, 80% \t train_loss: 0.14  took: 0.07s\n",
      "Epoch 27 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 27 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 27.\n",
      "Epoch 28 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 28 of 50, 20% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 28 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 28 of 50, 40% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 28 of 50, 50% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 28 of 50, 60% \t train_loss: 0.16  took: 0.06s\n",
      "Epoch 28 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 28 of 50, 80% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 28 of 50, 90% \t train_loss: 0.15  took: 0.06s\n",
      "Epoch 28 of 50, 100% \t train_loss: 0.14  took: 0.05s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 28.\n",
      "Epoch 29 of 50, 10% \t train_loss: 0.15  took: 0.07s\n",
      "Epoch 29 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 29 of 50, 30% \t train_loss: 0.13  took: 0.05s\n",
      "Epoch 29 of 50, 40% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 29 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 29 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 29 of 50, 70% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 29 of 50, 80% \t train_loss: 0.14  took: 0.06s\n",
      "Epoch 29 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 29 of 50, 100% \t train_loss: 0.14  took: 0.06s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 29.\n",
      "Epoch 30 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 30 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 30 of 50, 30% \t train_loss: 0.13  took: 0.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 of 50, 40% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 30 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 30 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 30 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 30 of 50, 80% \t train_loss: 0.14  took: 0.06s\n",
      "Epoch 30 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 30 of 50, 100% \t train_loss: 0.14  took: 0.05s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 30.\n",
      "Epoch 31 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 31 of 50, 20% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 31 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 31 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 31 of 50, 50% \t train_loss: 0.14  took: 0.07s\n",
      "Epoch 31 of 50, 60% \t train_loss: 0.16  took: 0.06s\n",
      "Epoch 31 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 31 of 50, 80% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 31 of 50, 90% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 31 of 50, 100% \t train_loss: 0.14  took: 0.06s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 31.\n",
      "Epoch 32 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 32 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 32 of 50, 30% \t train_loss: 0.13  took: 0.05s\n",
      "Epoch 32 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 32 of 50, 50% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 32 of 50, 60% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 32 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 32 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 32 of 50, 90% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 32 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 32.\n",
      "Epoch 33 of 50, 10% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 33 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 33 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 33 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 33 of 50, 50% \t train_loss: 0.14  took: 0.06s\n",
      "Epoch 33 of 50, 60% \t train_loss: 0.16  took: 0.06s\n",
      "Epoch 33 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 33 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 33 of 50, 90% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 33 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 33.\n",
      "Epoch 34 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 34 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 34 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 34 of 50, 40% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 34 of 50, 50% \t train_loss: 0.14  took: 0.06s\n",
      "Epoch 34 of 50, 60% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 34 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 34 of 50, 80% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 34 of 50, 90% \t train_loss: 0.15  took: 0.06s\n",
      "Epoch 34 of 50, 100% \t train_loss: 0.14  took: 0.06s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 34.\n",
      "Epoch 35 of 50, 10% \t train_loss: 0.15  took: 0.07s\n",
      "Epoch 35 of 50, 20% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 35 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 35 of 50, 40% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 35 of 50, 50% \t train_loss: 0.14  took: 0.06s\n",
      "Epoch 35 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 35 of 50, 70% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 35 of 50, 80% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 35 of 50, 90% \t train_loss: 0.15  took: 0.07s\n",
      "Epoch 35 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 35.\n",
      "Epoch 36 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 36 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 36 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 36 of 50, 40% \t train_loss: 0.14  took: 0.02s\n",
      "Epoch 36 of 50, 50% \t train_loss: 0.14  took: 0.02s\n",
      "Epoch 36 of 50, 60% \t train_loss: 0.16  took: 0.02s\n",
      "Epoch 36 of 50, 70% \t train_loss: 0.15  took: 0.02s\n",
      "Epoch 36 of 50, 80% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 36 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 36 of 50, 100% \t train_loss: 0.14  took: 0.02s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 36.\n",
      "Epoch 37 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 37 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 37 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 37 of 50, 40% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 37 of 50, 50% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 37 of 50, 60% \t train_loss: 0.16  took: 0.02s\n",
      "Epoch 37 of 50, 70% \t train_loss: 0.15  took: 0.02s\n",
      "Epoch 37 of 50, 80% \t train_loss: 0.14  took: 0.02s\n",
      "Epoch 37 of 50, 90% \t train_loss: 0.15  took: 0.02s\n",
      "Epoch 37 of 50, 100% \t train_loss: 0.14  took: 0.03s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 37.\n",
      "Epoch 38 of 50, 10% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 38 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 38 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 38 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 38 of 50, 50% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 38 of 50, 60% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 38 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 38 of 50, 80% \t train_loss: 0.14  took: 0.02s\n",
      "Epoch 38 of 50, 90% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 38 of 50, 100% \t train_loss: 0.14  took: 0.03s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 38.\n",
      "Epoch 39 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 39 of 50, 20% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 39 of 50, 30% \t train_loss: 0.13  took: 0.05s\n",
      "Epoch 39 of 50, 40% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 39 of 50, 50% \t train_loss: 0.14  took: 0.06s\n",
      "Epoch 39 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 39 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 39 of 50, 80% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 39 of 50, 90% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 39 of 50, 100% \t train_loss: 0.14  took: 0.05s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 39.\n",
      "Epoch 40 of 50, 10% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 40 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 40 of 50, 30% \t train_loss: 0.13  took: 0.05s\n",
      "Epoch 40 of 50, 40% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 40 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 40 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 40 of 50, 70% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 40 of 50, 80% \t train_loss: 0.14  took: 0.06s\n",
      "Epoch 40 of 50, 90% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 40 of 50, 100% \t train_loss: 0.14  took: 0.05s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 40.\n",
      "Epoch 41 of 50, 10% \t train_loss: 0.15  took: 0.06s\n",
      "Epoch 41 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 41 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 41 of 50, 40% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 41 of 50, 50% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 41 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 41 of 50, 70% \t train_loss: 0.15  took: 0.07s\n",
      "Epoch 41 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 41 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 41 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 41.\n",
      "Epoch 42 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 42 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 42 of 50, 30% \t train_loss: 0.13  took: 0.07s\n",
      "Epoch 42 of 50, 40% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 42 of 50, 50% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 42 of 50, 60% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 42 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 42 of 50, 80% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 42 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 42 of 50, 100% \t train_loss: 0.14  took: 0.07s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 42.\n",
      "Epoch 43 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 43 of 50, 20% \t train_loss: 0.16  took: 0.06s\n",
      "Epoch 43 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 43 of 50, 40% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 43 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 43 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 43 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 43 of 50, 80% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 43 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 43 of 50, 100% \t train_loss: 0.14  took: 0.05s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 43.\n",
      "Epoch 44 of 50, 10% \t train_loss: 0.15  took: 0.06s\n",
      "Epoch 44 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 44 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 44 of 50, 40% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 44 of 50, 50% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 44 of 50, 60% \t train_loss: 0.16  took: 0.02s\n",
      "Epoch 44 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 44 of 50, 80% \t train_loss: 0.14  took: 0.06s\n",
      "Epoch 44 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 44 of 50, 100% \t train_loss: 0.14  took: 0.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 44.\n",
      "Epoch 45 of 50, 10% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 45 of 50, 20% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 45 of 50, 30% \t train_loss: 0.13  took: 0.05s\n",
      "Epoch 45 of 50, 40% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 45 of 50, 50% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 45 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 45 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 45 of 50, 80% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 45 of 50, 90% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 45 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 45.\n",
      "Epoch 46 of 50, 10% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 46 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 46 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 46 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 46 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 46 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 46 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 46 of 50, 80% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 46 of 50, 90% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 46 of 50, 100% \t train_loss: 0.14  took: 0.05s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 46.\n",
      "Epoch 47 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 47 of 50, 20% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 47 of 50, 30% \t train_loss: 0.13  took: 0.05s\n",
      "Epoch 47 of 50, 40% \t train_loss: 0.14  took: 0.06s\n",
      "Epoch 47 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 47 of 50, 60% \t train_loss: 0.16  took: 0.03s\n",
      "Epoch 47 of 50, 70% \t train_loss: 0.15  took: 0.06s\n",
      "Epoch 47 of 50, 80% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 47 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 47 of 50, 100% \t train_loss: 0.14  took: 0.06s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 47.\n",
      "Epoch 48 of 50, 10% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 48 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 48 of 50, 30% \t train_loss: 0.13  took: 0.03s\n",
      "Epoch 48 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 48 of 50, 50% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 48 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 48 of 50, 70% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 48 of 50, 80% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 48 of 50, 90% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 48 of 50, 100% \t train_loss: 0.14  took: 0.03s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 48.\n",
      "Epoch 49 of 50, 10% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 49 of 50, 20% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 49 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 49 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 49 of 50, 50% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 49 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 49 of 50, 70% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 49 of 50, 80% \t train_loss: 0.14  took: 0.05s\n",
      "Epoch 49 of 50, 90% \t train_loss: 0.15  took: 0.04s\n",
      "Epoch 49 of 50, 100% \t train_loss: 0.14  took: 0.04s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 49.\n",
      "Epoch 50 of 50, 10% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 50 of 50, 20% \t train_loss: 0.16  took: 0.05s\n",
      "Epoch 50 of 50, 30% \t train_loss: 0.13  took: 0.04s\n",
      "Epoch 50 of 50, 40% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 50 of 50, 50% \t train_loss: 0.14  took: 0.03s\n",
      "Epoch 50 of 50, 60% \t train_loss: 0.16  took: 0.04s\n",
      "Epoch 50 of 50, 70% \t train_loss: 0.15  took: 0.05s\n",
      "Epoch 50 of 50, 80% \t train_loss: 0.14  took: 0.04s\n",
      "Epoch 50 of 50, 90% \t train_loss: 0.15  took: 0.03s\n",
      "Epoch 50 of 50, 100% \t train_loss: 0.14  took: 0.05s\n",
      "val_loss = 0.15\n",
      "Snapshot saved at epoch 50.\n",
      "Training done! A list of 50 models saved.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing standard benchmark p-values for 500 test points...\n",
      "Calibrating each model in the list...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ConvAutoencoder:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"t_conv1.weight\", \"t_conv1.bias\", \"t_conv2.weight\", \"t_conv2.bias\". \n\tUnexpected key(s) in state_dict: \"base_model.0.weight\", \"base_model.0.bias\", \"base_model.2.weight\", \"base_model.2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Change random seed for this repetition\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     seed \u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m---> 25\u001b[0m     results_new \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_train_bm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_val_bm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_cal_bm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#     results_new = add_header(results_new)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     results_new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepetition\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m r\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(seed, n_train_bm, n_val_bm, n_cal_bm, n_test, lr, n_epoch, alpha_list, batch_size, num_worker, visualize)\u001b[0m\n\u001b[0;32m     50\u001b[0m best_loss_bm, best_model_bm, val_loss_history_bm \u001b[38;5;241m=\u001b[39m CES_oc_bm\u001b[38;5;241m.\u001b[39mselect_model()\n\u001b[0;32m     51\u001b[0m model_list_bm \u001b[38;5;241m=\u001b[39m CES_oc_bm\u001b[38;5;241m.\u001b[39mmodel_list\n\u001b[1;32m---> 52\u001b[0m C_PVals_bm \u001b[38;5;241m=\u001b[39m \u001b[43mConformal_PVals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_bm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal_loader_bm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_list_bm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m pvals_bm \u001b[38;5;241m=\u001b[39m C_PVals_bm\u001b[38;5;241m.\u001b[39mcompute_pvals(inputs, [best_model_bm]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m     54\u001b[0m results_bm \u001b[38;5;241m=\u001b[39m eval_pvalues(pvals_bm, labels, alpha_list)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Conformalized_early_stopping\\notebook_examples\\../ConformalizedES\\inference.py:168\u001b[0m, in \u001b[0;36mConformal_PVals.__init__\u001b[1;34m(self, net, device, cal_loader, model_list, verbose, progress, random_state)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalibrating each model in the list...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calibrate_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialization done!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Conformalized_early_stopping\\notebook_examples\\../ConformalizedES\\inference.py:185\u001b[0m, in \u001b[0;36mConformal_PVals._calibrate_scores\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_idx \u001b[38;5;129;01min\u001b[39;00m iterator:      \n\u001b[0;32m    184\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_list[model_idx]\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcal_loader:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1599\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1600\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1601\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1605\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ConvAutoencoder:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"t_conv1.weight\", \"t_conv1.bias\", \"t_conv2.weight\", \"t_conv2.bias\". \n\tUnexpected key(s) in state_dict: \"base_model.0.weight\", \"base_model.0.bias\", \"base_model.2.weight\", \"base_model.2.bias\". "
     ]
    }
   ],
   "source": [
    "# Initialize result data frame\n",
    "results = pd.DataFrame({})\n",
    "\n",
    "# Define the experiment parameters\n",
    "n_train_bm = 100\n",
    "n_val_bm = 100\n",
    "n_cal_bm = 100\n",
    "n_test = 500\n",
    "lr = 1\n",
    "n_epoch = 50\n",
    "alpha_list = [0.1]\n",
    "num_repetitions = 10\n",
    "\n",
    "def add_header(df):\n",
    "    df[\"n_epoch\"] = n_epoch\n",
    "    df[\"n_calib\"] = n_cal_bm\n",
    "    return df\n",
    "\n",
    "for r in range(num_repetitions):\n",
    "    print(\"\\nStarting repetition {:d} of {:d}:\\n\".format(r+1, num_repetitions))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Change random seed for this repetition\n",
    "    seed = r\n",
    "    results_new = run_experiment(seed, n_train_bm, n_val_bm, n_cal_bm, n_test, lr, n_epoch, alpha_list)\n",
    "#     results_new = add_header(results_new)\n",
    "    results_new[\"Repetition\"] = r\n",
    "    results = pd.concat([results, results_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db3ec1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5404b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_results = results[results[\"Method\"]==\"Standard benchmark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b745a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_results = results[results[\"Method\"]==\"Naive benchmark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a34fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The standard benchmark FPR is: {:.6f}, TPR is:{:.6f}\".format(np.sum(sb_results[\"Fixed-FPR\"])/num_repetitions, \n",
    "                                                     np.sum(sb_results[\"Fixed-TPR\"])/num_repetitions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba23697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The naive benchmark FPR is: {:.6f}, TPR is:{:.6f}\".format(np.sum(nb_results[\"Fixed-FPR\"])/num_repetitions,\n",
    "                                                  np.sum(nb_results[\"Fixed-TPR\"])/num_repetitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "sns.boxplot(y=\"Fixed-FPR\", x=\"Method\", hue=\"Method\", data=results, ax=ax1)\n",
    "ax1.set(xlabel='Method', ylabel='FPR')\n",
    "ax1.axhline(alpha_list[0], ls='--', color=\"red\")\n",
    "ax1.legend(loc='upper center', title='Method')\n",
    "\n",
    "sns.boxplot(y=\"Fixed-TPR\", x=\"Method\", hue=\"Method\", data=results, ax=ax2)\n",
    "ax2.set(xlabel='Method', ylabel='TPR')\n",
    "ax2.legend(loc='upper center', title='Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results dataframe\n",
    "from pathlib import Path\n",
    "results_dir = \"./results/\"\n",
    "Path(results_dir).mkdir(parents=True, exist_ok=True)\n",
    "results.to_csv(results_dir+'OD_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc742a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
