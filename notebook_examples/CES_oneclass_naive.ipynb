{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c990a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af907b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Colors from Colorbrewer Paired_12\n",
    "colors = [[31, 120, 180], [51, 160, 44], [250,159,181]]\n",
    "colors = [(r / 255, g / 255, b / 255) for (r, g, b) in colors]\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img, norm):\n",
    "    \"\"\"\n",
    "    :param img: (PyTorch Tensor)\n",
    "    \"\"\"\n",
    "    if norm:\n",
    "        # unnormalize\n",
    "        img = img / 2 + 0.5    \n",
    "        \n",
    "    # Convert tensor to numpy array\n",
    "    npimg = img.numpy()\n",
    "    # Color channel first -> color channel last\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def get_image(img):\n",
    "    \"\"\"\n",
    "    convert a tensor to images suitable for plotting\n",
    "    :param img: (PyTorch Tensor)\n",
    "    \"\"\"\n",
    "    # unnormalize\n",
    "    img = img / 2 + 0.5     \n",
    "    # Convert tensor to numpy array\n",
    "    npimg = img.numpy()\n",
    "    # Color channel first -> color channel last\n",
    "    return np.transpose(npimg, (1, 2, 0))\n",
    "\n",
    "def plot_loss_acc(train_loss, val_loss, train_acc, val_acc):\n",
    "    x = np.arange(1, len(train_loss) + 1)\n",
    "\n",
    "    fig,axs = plt.subplots(1, 2, figsize=(16,6))\n",
    "    axs[0].plot(x, train_loss, color=colors[0], label=\"Training loss\", linewidth=2)\n",
    "    axs[0].plot(x, val_loss, color=colors[1], label=\"Validation loss\", linewidth=2)\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend(loc='upper right')\n",
    "    axs[0].set_title(\"Evolution of the training, validation and test loss\")\n",
    "\n",
    "    axs[1].plot(x, train_acc, color=colors[0], label=\"Training accuracy\", linewidth=2)\n",
    "    axs[1].plot(x, val_acc, color=colors[1], label=\"Validation accuracy\", linewidth=2)\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend(loc='lower right')\n",
    "    axs[1].set_title(\"Evolution of the training, validation and test accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_loss(train_loss, val_loss):\n",
    "    x = np.arange(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, train_loss, color=colors[0], label=\"Training loss\", linewidth=2)\n",
    "    plt.plot(x, val_loss, color=colors[1], label=\"Validation loss\", linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"Evolution of the training, validation and test loss\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_pvals(pvals, labels):\n",
    "    idx_in = np.where(labels==0)[0]\n",
    "    idx_out = np.where(labels==1)[0]\n",
    "\n",
    "    pvals_in = np.array(pvals)[idx_in]\n",
    "    pvals_out = np.array(pvals)[idx_out]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.hist(pvals_in, bins=100, alpha=0.5, label=\"inliers\")\n",
    "    plt.hist(pvals_out, bins=100, alpha=0.5, label=\"outliers\")\n",
    "    plt.legend()\n",
    "    plt.show\n",
    "\n",
    "    print('Average p-value for inliers is {:3f}, average p-value for outliers is {:3f}.'\\\n",
    "          .format(np.mean(pvals_in), np.mean(pvals_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8f3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "sys.path.append('../ConformalizedES')\n",
    "sys.path.append('../third_party')\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b1132",
   "metadata": {},
   "source": [
    "# Experiment: Compare with the naive benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef027e",
   "metadata": {},
   "source": [
    "### Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0c50ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MNIST Dataset\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#      transforms.Normalize(mean=0.5, std=0.5)])\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "train_set_full = datasets.MNIST(root = \"./data\", train = True, download = True, transform=transform)\n",
    "test_set_full = datasets.MNIST(root = \"./data\", train = False, download = True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd1674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of available training data is: 5923.\n",
      "total number of test data is 1954 in which 980 are label 0 test data, 974 are label 8 test data.\n"
     ]
    }
   ],
   "source": [
    "from datasetMaker import get_class_i, DatasetMaker\n",
    "\n",
    "x_train_full = train_set_full.data\n",
    "y_train_full = train_set_full.targets\n",
    "x_test_full = test_set_full.data\n",
    "y_test_full = test_set_full.targets\n",
    "\n",
    "# Train set composed only of number 0\n",
    "train_set = \\\n",
    "    DatasetMaker(\n",
    "        [get_class_i(x_train_full, y_train_full, 0)]\n",
    "    )\n",
    "\n",
    "# Test set is a mixture of number 0 and 8\n",
    "test_set = \\\n",
    "    DatasetMaker(\n",
    "        [get_class_i(x_test_full, y_test_full, 0),\n",
    "        get_class_i(x_test_full, y_test_full, 8),\n",
    "]\n",
    "    )\n",
    "\n",
    "print('total number of available training data is: {:d}.'.format(len(train_set)))\n",
    "print('total number of test data is {:d} in which {:d} are label 0 test data, {:d} are label 8 test data.'\\\n",
    "      .format(len(test_set), test_set.lengths[0],test_set.lengths[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86fabe3",
   "metadata": {},
   "source": [
    "### Benchmark data-splitting vs CES data-splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11925c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "def split_data(seed, n_train_bm, n_val_bm, n_cal_bm, batch_size=10, num_workers=0):\n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "\n",
    "    # MNIST is a simple dataset, hence we only take 300 inlier data to demonstrate\n",
    "    # Benchmark data splitting: equally split the data into 3 sets\n",
    "    n_full = len(train_set)\n",
    "    n_data = n_train_bm + n_val_bm + n_cal_bm\n",
    "\n",
    "    train_set_bm, val_set_bm, cal_set_bm, _ = th.utils.data.random_split(train_set,\\\n",
    "                                     [n_train_bm, n_val_bm, n_cal_bm, n_full-n_data])\n",
    "\n",
    "    # CES data splitting: calibration set is not needed, merge back to the training set\n",
    "    n_train_ces = n_train_bm + n_val_bm\n",
    "    n_val_ces = n_cal_bm\n",
    "\n",
    "    train_set_ces, val_set_ces, _ = th.utils.data.random_split(train_set,\\\n",
    "                                     [n_train_ces, n_val_ces, n_full-n_data])\n",
    "    \n",
    "    train_loader_bm = th.utils.data.DataLoader(train_set_bm, batch_size=batch_size,\n",
    "                                          num_workers=num_workers)\n",
    "\n",
    "    val_loader_bm = th.utils.data.DataLoader(val_set_bm, batch_size=n_val_bm,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "    cal_loader_bm = th.utils.data.DataLoader(cal_set_bm, batch_size=n_cal_bm,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "    # For CES\n",
    "    train_loader_ces = th.utils.data.DataLoader(train_set_ces, batch_size=batch_size,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "    val_loader_ces = th.utils.data.DataLoader(val_set_ces, batch_size=n_val_ces,\n",
    "                                              num_workers=num_workers)\n",
    "    return train_loader_bm, val_loader_bm, cal_loader_bm, train_loader_ces, val_loader_ces\n",
    "\n",
    "def load_test(seed, n_test, num_workers=0):\n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "\n",
    "    test_sampler = SubsetRandomSampler(np.arange(n_test, dtype=np.int64))\n",
    "    test_loader = th.utils.data.DataLoader(test_set, batch_size=n_test, sampler=test_sampler,\n",
    "                                             num_workers=num_workers)\n",
    "\n",
    "    # get all test images\n",
    "    dataiter = iter(test_loader)\n",
    "    inputs, labels = dataiter.next()\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b3078",
   "metadata": {},
   "source": [
    "## Define experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b20f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from method import CES_oneClass\n",
    "from networks import ConvAutoencoder\n",
    "from utils import eval_pvalues\n",
    "import torch.optim as optim\n",
    "from inference import Conformal_PVals\n",
    "\n",
    "\n",
    "def run_experiment(seed, n_train_bm, n_val_bm, n_cal_bm, n_test, lr, n_epoch, alpha_list, batch_size=10, num_worker=0, visualize=True):\n",
    "    # Initialize result data frame\n",
    "    results = pd.DataFrame({})\n",
    "    \n",
    "    # Get the dataloaders and test points\n",
    "    train_loader_bm, val_loader_bm, cal_loader_bm, train_loader_ces, val_loader_ces =\\\n",
    "    split_data(seed, n_train_bm, n_val_bm, n_cal_bm, batch_size, num_worker)\n",
    "    inputs, labels = load_test(seed, n_test)\n",
    "    \n",
    "    \n",
    "    # Define default device, we should use the GPU (cuda) if available\n",
    "    device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")### Define subset of the dataset (so it is faster to train)\n",
    "    if th.cuda.is_available():\n",
    "        # Make CuDNN Determinist\n",
    "        th.backends.cudnn.deterministic = True\n",
    "        th.cuda.manual_seed(seed)\n",
    "    # Set the NN parameters \n",
    "    net_bm = ConvAutoencoder()\n",
    "    Loss = th.nn.MSELoss()\n",
    "    def criterion(outputs, inputs, targets):\n",
    "        return Loss(outputs, inputs)\n",
    "    optimizer_bm = optim.Adam(net_bm.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    \n",
    "    # Train with benchmark data splitting\n",
    "    print(\"Training with standard data splitting...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    CES_oc_bm = CES_oneClass(net_bm, device, train_loader_bm, batch_size=batch_size, max_epoch=n_epoch, \n",
    "                        learning_rate=lr, val_loader=val_loader_bm, criterion=criterion,optimizer=optimizer_bm)\n",
    "    CES_oc_bm.full_train(save_dir = './models/oneClass/exp'+str(seed)+'/benchmarks/', save_every = 1)\n",
    "    \n",
    "    if visualize:\n",
    "        plot_loss(CES_oc_bm.train_loss_history, CES_oc_bm.val_loss_history)\n",
    "    \n",
    "    # Compute the benchmark p-values\n",
    "    print('Computing standard benchmark p-values for {:d} test points...'.format(n_test))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    best_loss_bm, best_model_bm, val_loss_history_bm = CES_oc_bm.select_model()\n",
    "    model_list_bm = CES_oc_bm.model_list\n",
    "    C_PVals_bm = Conformal_PVals(net_bm, device, cal_loader_bm, model_list_bm, random_state = seed)\n",
    "    pvals_bm = C_PVals_bm.compute_pvals(inputs, [best_model_bm]*len(inputs))\n",
    "    results_bm = eval_pvalues(pvals_bm, labels, alpha_list)\n",
    "    results_bm[\"Method\"] = \"Standard benchmark\"\n",
    "    results = pd.concat([results, results_bm])\n",
    "    \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "\n",
    "    net_ces = ConvAutoencoder()\n",
    "    optimizer_ces = optim.Adam(net_ces.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize the CES class with model parameters\n",
    "    print(\"Training with CES data splitting...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    CES_oc_ces = CES_oneClass(net_ces, device, train_loader_ces, batch_size=batch_size, max_epoch=n_epoch, \n",
    "                            learning_rate=lr, val_loader=val_loader_ces, criterion=criterion,optimizer=optimizer_ces)\n",
    "    CES_oc_ces.full_train(save_dir = './models/oneClass/exp'+str(seed)+'/ces/', save_every = 1)\n",
    "    \n",
    "    if visualize:\n",
    "        plot_loss(CES_oc_ces.train_loss_history, CES_oc_ces.val_loss_history)\n",
    "    \n",
    "    model_list_ces = CES_oc_ces.model_list\n",
    "    C_PVals_ces = Conformal_PVals(net_ces, device, val_loader_ces, model_list_ces, random_state = seed)\n",
    "    \n",
    "    # Compute the benchmark p-values\n",
    "    print('Computing naive benchmark p-values for {:d} test points...'.format(n_test))\n",
    "    best_loss_naive, best_model_naive, val_loss_history_naive = CES_oc_ces.select_model()\n",
    "    pvals_naive = C_PVals_ces.compute_pvals(inputs, [best_model_naive]*len(inputs))\n",
    "    results_naive = eval_pvalues(pvals_naive, labels, alpha_list)\n",
    "    results_naive[\"Method\"] = \"Naive benchmark\"\n",
    "    results = pd.concat([results, results_naive])\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3265b6fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting repetition 1 of 10:\n",
      "\n",
      "Training with standard data splitting...\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 10\n",
      "n_epochs= 50\n",
      "learning_rate= 1\n",
      "==============================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Either `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...) and `preds` should be (N, C, ...).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Change random seed for this repetition\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     seed \u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m---> 25\u001b[0m     results_new \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_train_bm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_val_bm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_cal_bm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#     results_new = add_header(results_new)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     results_new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepetition\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m r\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(seed, n_train_bm, n_val_bm, n_cal_bm, n_test, lr, n_epoch, alpha_list, batch_size, num_worker, visualize)\u001b[0m\n\u001b[0;32m     37\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m     39\u001b[0m CES_oc_bm \u001b[38;5;241m=\u001b[39m CES_oneClass(net_bm, device, train_loader_bm, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, max_epoch\u001b[38;5;241m=\u001b[39mn_epoch, \n\u001b[0;32m     40\u001b[0m                     learning_rate\u001b[38;5;241m=\u001b[39mlr, val_loader\u001b[38;5;241m=\u001b[39mval_loader_bm, criterion\u001b[38;5;241m=\u001b[39mcriterion,optimizer\u001b[38;5;241m=\u001b[39moptimizer_bm)\n\u001b[1;32m---> 41\u001b[0m \u001b[43mCES_oc_bm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/oneClass/exp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/benchmarks/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n\u001b[0;32m     44\u001b[0m     plot_loss(CES_oc_bm\u001b[38;5;241m.\u001b[39mtrain_loss_history, CES_oc_bm\u001b[38;5;241m.\u001b[39mval_loss_history)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Conformalized_early_stopping\\notebook_examples\\../ConformalizedES\\method.py:123\u001b[0m, in \u001b[0;36mConformalizedES.full_train\u001b[1;34m(self, save_dir, save_every)\u001b[0m\n\u001b[0;32m    121\u001b[0m epoch \u001b[38;5;241m=\u001b[39m saving_epoch \u001b[38;5;241m-\u001b[39m save_every \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m i\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macc:\n\u001b[1;32m--> 123\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_single_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_single_epoch(epoch)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Conformalized_early_stopping\\notebook_examples\\../ConformalizedES\\method.py:74\u001b[0m, in \u001b[0;36mConformalizedES.train_single_epoch\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     71\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macc:\n\u001b[1;32m---> 74\u001b[0m     running_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     75\u001b[0m     total_train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(accuracy(outputs,targets)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# print every 10th of epoch\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchmetrics\\functional\\classification\\accuracy.py:411\u001b[0m, in \u001b[0;36maccuracy\u001b[1;34m(preds, target, average, mdmc_average, threshold, top_k, subset_accuracy, num_classes, multiclass, ignore_index)\u001b[0m\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `top_k` should be an integer larger than 0, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    410\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _input_squeeze(preds, target)\n\u001b[1;32m--> 411\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[43m_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulticlass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m reduce \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;28;01melse\u001b[39;00m average\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subset_accuracy \u001b[38;5;129;01mand\u001b[39;00m _check_subset_validity(mode):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchmetrics\\functional\\classification\\accuracy.py:59\u001b[0m, in \u001b[0;36m_mode\u001b[1;34m(preds, target, threshold, top_k, num_classes, multiclass, ignore_index)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mode\u001b[39m(\n\u001b[0;32m     30\u001b[0m     preds: Tensor,\n\u001b[0;32m     31\u001b[0m     target: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     ignore_index: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataType:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124;03m\"\"\"Finds the mode of the input tensors.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m        <DataType.MULTICLASS: 'multi-class'>\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[43m_check_classification_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulticlass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulticlass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mode\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchmetrics\\utilities\\checks.py:271\u001b[0m, in \u001b[0;36m_check_classification_inputs\u001b[1;34m(preds, target, threshold, num_classes, multiclass, top_k, ignore_index)\u001b[0m\n\u001b[0;32m    268\u001b[0m _basic_input_validation(preds, target, threshold, multiclass, ignore_index)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Check that shape/types fall into one of the cases\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m case, implied_classes \u001b[38;5;241m=\u001b[39m \u001b[43m_check_shape_and_type_consistency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# Check consistency with the `C` dimension in case of multi-class data\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m target\u001b[38;5;241m.\u001b[39mshape:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchmetrics\\utilities\\checks.py:117\u001b[0m, in \u001b[0;36m_check_shape_and_type_consistency\u001b[1;34m(preds, target)\u001b[0m\n\u001b[0;32m    115\u001b[0m         case \u001b[38;5;241m=\u001b[39m DataType\u001b[38;5;241m.\u001b[39mMULTIDIM_MULTICLASS\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and `preds` should be (N, C, ...).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m     )\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m case, implied_classes\n",
      "\u001b[1;31mValueError\u001b[0m: Either `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...) and `preds` should be (N, C, ...)."
     ]
    }
   ],
   "source": [
    "# Initialize result data frame\n",
    "results = pd.DataFrame({})\n",
    "\n",
    "# Define the experiment parameters\n",
    "n_train_bm = 100\n",
    "n_val_bm = 100\n",
    "n_cal_bm = 100\n",
    "n_test = 500\n",
    "lr = 1\n",
    "n_epoch = 50\n",
    "alpha_list = [0.1]\n",
    "num_repetitions = 10\n",
    "\n",
    "def add_header(df):\n",
    "    df[\"n_epoch\"] = n_epoch\n",
    "    df[\"n_calib\"] = n_cal_bm\n",
    "    return df\n",
    "\n",
    "for r in range(num_repetitions):\n",
    "    print(\"\\nStarting repetition {:d} of {:d}:\\n\".format(r+1, num_repetitions))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Change random seed for this repetition\n",
    "    seed = r\n",
    "    results_new = run_experiment(seed, n_train_bm, n_val_bm, n_cal_bm, n_test, lr, n_epoch, alpha_list)\n",
    "#     results_new = add_header(results_new)\n",
    "    results_new[\"Repetition\"] = r\n",
    "    results = pd.concat([results, results_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db3ec1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5404b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_results = results[results[\"Method\"]==\"Standard benchmark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b745a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_results = results[results[\"Method\"]==\"Naive benchmark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a34fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The standard benchmark FPR is: {:.6f}, TPR is:{:.6f}\".format(np.sum(sb_results[\"Fixed-FPR\"])/num_repetitions, \n",
    "                                                     np.sum(sb_results[\"Fixed-TPR\"])/num_repetitions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba23697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The naive benchmark FPR is: {:.6f}, TPR is:{:.6f}\".format(np.sum(nb_results[\"Fixed-FPR\"])/num_repetitions,\n",
    "                                                  np.sum(nb_results[\"Fixed-TPR\"])/num_repetitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "sns.boxplot(y=\"Fixed-FPR\", x=\"Method\", hue=\"Method\", data=results, ax=ax1)\n",
    "ax1.set(xlabel='Method', ylabel='FPR')\n",
    "ax1.axhline(alpha_list[0], ls='--', color=\"red\")\n",
    "ax1.legend(loc='upper center', title='Method')\n",
    "\n",
    "sns.boxplot(y=\"Fixed-TPR\", x=\"Method\", hue=\"Method\", data=results, ax=ax2)\n",
    "ax2.set(xlabel='Method', ylabel='TPR')\n",
    "ax2.legend(loc='upper center', title='Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results dataframe\n",
    "from pathlib import Path\n",
    "results_dir = \"./results/\"\n",
    "Path(results_dir).mkdir(parents=True, exist_ok=True)\n",
    "results.to_csv(results_dir+'OD_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc742a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
